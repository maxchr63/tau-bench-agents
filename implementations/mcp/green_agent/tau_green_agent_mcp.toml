name = "tau_green_agent_mcp"
description = "The assessment hosting agent for tau-bench (refactored with AgentBeats SDK)."
version = "0.1.0"
battle_timeout = 1050

defaultInputModes = ["text"]
defaultOutputModes = ["text"]
[capabilities]
streaming = true

# Pass@k evaluation configuration
[pass_k_config]
mode = "manual"           # "manual" or "random"
k = 4                     # Number of attempts per task (must be even for k/2 calculation)
domain = "retail"         # For manual mode: "retail" or "airline"
task_id = 1               # For manual mode: specific task to test (0-based index)
num_battles = 5           # For random mode: how many different random tasks to test

[[skills]]
id = "host_assess_tau_bench"
name = "Tau-bench assessment hosting"
description = """
Assess the tool-calling ability of an agent.
"""
tags = ["green agent", "assessment hosting", "tau-bench"]
examples = ["""    
Your task is to instantiate tau-bench to test the agent located at:
<white_agent_url>
http://localhost:9004/
</white_agent_url>
You should use the following env configuration:
<env_config>
{
  "env": "retail",
  "user_strategy": "llm",
  "user_model": "USE_PROVIDER config in tools.py controls this",
  "user_provider": "Automatically set based on USE_PROVIDER",
  "task_split": "test",
  "task_ids": [5]
}
</env_config>

Note: LLM provider is controlled by USE_PROVIDER in green_agent/tools.py
Currently set to: openrouter (using openai/gpt-5-nano)
    """]
